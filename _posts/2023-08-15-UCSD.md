---
title: Obstacle Avoidance
description: Safe autonomous navigation of ground robots using depth sensing.
author: Leo Sun
date: 2023-08-15
image:
  path: /assets/img/20230815UCSD/pybullet.png
  alt: Obstacle Avoidance
---

## Overview
Obstacle avoidance is a core capability for autonomous robots operating in real-world environments. Applications range from hospital robots navigating crowded hallways, to indoor delivery and cleaning systems, to autonomous vehicles and search-and-rescue platforms operating in hazardous conditions.

This project focuses on developing and evaluating depth-camera–based obstacle avoidance algorithms for ground robots. Our goal was to explore how different perception and planning pipelines affect safety and performance, and to understand the tradeoffs between local reactive methods and global planning approaches.

---

## Problem Setup
The robot operates in a simulated environment built using PyBullet, consisting of:
- An empty plane
- Randomly generated cylindrical obstacles

The robot follows a standard Ackermann drive model, parameterized by:
- Position and orientation
- Steering angle
- Linear velocity

A forward-facing depth camera provides distance measurements to obstacles directly ahead. The objective is to navigate safely around obstacles while maintaining a reasonable speed.

Performance is evaluated using:
- Average distance traveled without collision
- Average speed

---

## Method 1: Planning-Based Obstacle Avoidance

### Perception Pipeline
The planning-based approach follows a multi-stage pipeline:
1. Receive depth images from the camera
2. Convert depth images into a 3D point cloud
3. Project the point cloud into a 2D occupancy map
4. Inflate obstacles in the map to maintain safe clearance
5. Use A\* to plan a collision-free path

<!-- IMAGE: Depth image, Point cloud, 2D map side by side -->
<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 1rem; align-items: flex-start;">
  <figure style="text-align: center; margin: 0;">
    <img src="/assets/img/20230815UCSD/depthimage.png" alt="Depth image" style="height: 180px; width: auto;">
    <figcaption class="text-center pt-2 pb-2">Depth image</figcaption>
  </figure>
  <figure style="text-align: center; margin: 0;">
    <img src="/assets/img/20230815UCSD/pointcloud.png" alt="Point cloud" style="height: 180px; width: auto;">
    <figcaption class="text-center pt-2 pb-2">Point cloud</figcaption>
  </figure>
  <figure style="text-align: center; margin: 0;">
    <img src="/assets/img/20230815UCSD/2dmap.png" alt="2D map" style="height: 180px; width: auto;">
    <figcaption class="text-center pt-2 pb-2">2D occupancy map</figcaption>
  </figure>
</div>

<!-- GIF: A* planning -->
![A* planning GIF](/assets/img/20230815UCSD/astar.gif)
_A* path planning with inflated obstacles_

---

### Path Following
Once a path is generated:
- The robot follows it using a P controller
- The controller steers the robot toward successive waypoints
- Velocity is held constant during execution

This approach emphasizes global structure and explicit safety margins but depends heavily on map accuracy.

---

## Method 2: Local Reactive Obstacle Avoidance
We also implemented a simpler, purely reactive method that does not rely on mapping or global planning.

### Depth-Based Steering
- The depth image is split into left and right halves
- Depth values in each half are summed
- The robot steers toward the side with greater free space (larger depth sum)

This approach is lightweight, fast, and avoids errors introduced by imperfect mapping.

---

## Optimal Target Selection
To improve steering behavior, we computed an optimal target direction by:
- Identifying gaps large enough for the robot to pass through
- Selecting the gap closest to the robot’s current trajectory
- Minimizing unnecessary lateral movement

<!-- IMAGE: Optimal target calculation -->
![Optimal target placeholder](/assets/img/20230815UCSD/optimaltarget.png)
_Selection of optimal target gap for navigation_

---

## Results & Evaluation
Each algorithm was tested over 20 independent trials.

### Performance Metrics

Local Reactive Avoidance
- Average distance traveled without collision: 19.56 m
- Average speed: 0.52 m/s

Planning + Local Avoidance
- Average distance traveled without collision: 13.33 m
- Average speed: 0.29 m/s

<!-- IMAGE: Results graph -->
![Results graph placeholder](/assets/img/20230815UCSD/results.png)
_Comparison of performance with and without planning_

Surprisingly, the local-only method outperformed the combined planning approach. We believe this is primarily due to map inaccuracies caused by PyBullet camera intrinsics, which made it difficult to construct a perfectly accurate local map. These inaccuracies occasionally caused collisions despite planned paths.

---

## Discussion
This project highlights an important insight in robotics:
> More complex pipelines do not always lead to better real-world performance.

While planning-based approaches provide structure and interpretability, they are sensitive to perception errors. In contrast, simple reactive methods can be more robust when sensing is noisy or imperfect.

---

## Future Work
Potential directions for improvement include:
- Improved map accuracy through better camera calibration and higher-resolution maps
- Advanced path-following controllers, such as Pure Pursuit combined with PID control
- Adaptive velocity control that slows near obstacles and accelerates on clear paths
- Dynamic environments with moving obstacles of varying shapes and speeds

---

## Demo

<!-- VIDEO: Full demo video -->
{% include embed/youtube.html id='sip1lFOLpFY' %}
_Full demonstration of obstacle avoidance behaviors_